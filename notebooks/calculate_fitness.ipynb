{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b074e728-74e2-46af-874d-af7a5352e0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import stats\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "import time\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44e7ab0f-abcd-4320-b488-4e378c542047",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def fast_linear_regression(x_arrays, y_arrays):\n",
    "#     \"\"\"\n",
    "#     Compute slopes, intercepts, and their standard errors for n-point datasets.\n",
    "\n",
    "#     This function uses a vectorized approach based on the analytical solutions\n",
    "#     for ordinary least squares (OLS) regression parameters and their standard\n",
    "#     errors.\n",
    "\n",
    "#     params:\n",
    "#         x_arrays (np.ndarray): A 2D NumPy array of x-coordinates, with shape\n",
    "#             (n_datasets, n_points).\n",
    "#         y_arrays (np.ndarray): A 2D NumPy array of y-coordinates, with shape\n",
    "#             (n_datasets, n_points).\n",
    "\n",
    "#     returns:\n",
    "#         tuple: A tuple containing four 1D NumPy arrays:\n",
    "#             - slopes (np.ndarray): The calculated slope for each dataset.\n",
    "#             - intercepts (np.ndarray): The calculated intercept for each\n",
    "#                 dataset.\n",
    "#             - se_slopes (np.ndarray): The standard error of the slope for each\n",
    "#                 dataset.\n",
    "#             - se_intercepts (np.ndarray): The standard error of the intercept\n",
    "#                 for each dataset.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Ensure inputs are NumPy arrays\n",
    "#     x = np.asarray(x_arrays)\n",
    "#     y = np.asarray(y_arrays)\n",
    "\n",
    "#     # Get the number of points per fit (n) and degrees of freedom (df)\n",
    "#     n = x.shape[1]\n",
    "#     df = n - 2\n",
    "    \n",
    "#     # --- Step 1: Calculate Slope and Intercept ---\n",
    "#     x_mean = np.mean(x, axis=1)\n",
    "#     y_mean = np.mean(y, axis=1)\n",
    "\n",
    "#     # Sum of squares for x (Sxx) and sum of products for xy (Sxy)\n",
    "#     ss_xx = np.sum((x - x_mean[:, np.newaxis])**2, axis=1)\n",
    "#     ss_xy = np.sum((y - y_mean[:, np.newaxis]) * (x - x_mean[:, np.newaxis]), axis=1)\n",
    "\n",
    "#     # Calculate slopes (b1)\n",
    "#     # Handle the case where ss_xx is zero to avoid division by zero errors.\n",
    "#     slopes = np.divide(ss_xy, ss_xx, out=np.full_like(ss_xy, np.nan), where=ss_xx!=0)\n",
    "    \n",
    "#     # Calculate intercepts (b0)\n",
    "#     intercepts = y_mean - slopes * x_mean\n",
    "\n",
    "#     # --- Step 2: Calculate Standard Errors ---\n",
    "#     # Predicted y values and Residual Sum of Squares (RSS)\n",
    "#     y_pred = intercepts[:, np.newaxis] + slopes[:, np.newaxis] * x\n",
    "#     rss = np.sum((y - y_pred)**2, axis=1)\n",
    "\n",
    "#     # Residual Standard Error (RSE)\n",
    "#     if df > 0:\n",
    "#         rse = np.sqrt(rss / df)\n",
    "    \n",
    "#         # Standard Error of the Slope (SE_b1)\n",
    "#         se_slopes = rse / np.sqrt(ss_xx)\n",
    "    \n",
    "#         # Standard Error of the Intercept (SE_b0)\n",
    "#         term_in_sqrt = (1/n) + (x_mean**2 / ss_xx)\n",
    "#         se_intercepts = rse * np.sqrt(term_in_sqrt)\n",
    "#     else:\n",
    "#         se_slopes = np.nan*np.ones(len(slopes))\n",
    "#         se_intercepts = np.nan*np.ones(len(slopes))\n",
    "\n",
    "#     return slopes, intercepts, se_slopes, se_intercepts\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# def fast_weighted_linear_regression(x_arrays, y_arrays, y_err_arrays):\n",
    "#     \"\"\"\n",
    "#     Compute slopes, intercepts, and their standard errors for n-point datasets\n",
    "#     using weighted least squares (WLS).\n",
    "\n",
    "#     This function uses a vectorized approach based on the analytical solutions\n",
    "#     for WLS regression parameters and their standard errors, assuming the\n",
    "#     provided variances are known.\n",
    "\n",
    "#     params:\n",
    "#         x_arrays (np.ndarray): A 2D NumPy array of x-coordinates, with shape\n",
    "#             (n_datasets, n_points).\n",
    "#         y_arrays (np.ndarray): A 2D NumPy array of y-coordinates, with shape\n",
    "#             (n_datasets, n_points).\n",
    "#         y_err_arrays (np.ndarray): A 2D NumPy array of the variance (sigma^2)\n",
    "#             of each y measurement, with shape (n_datasets, n_points).\n",
    "\n",
    "#     returns:\n",
    "#         tuple: A tuple containing four 1D NumPy arrays:\n",
    "#             - slopes (np.ndarray): The calculated slope for each dataset.\n",
    "#             - intercepts (np.ndarray): The calculated intercept for each dataset.\n",
    "#             - se_slopes (np.ndarray): The standard error of the slope for each dataset.\n",
    "#             - se_intercepts (np.ndarray): The standard error of the intercept for each dataset.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # --- Step 0: Data Preparation ---\n",
    "#     x = np.asarray(x_arrays)\n",
    "#     y = np.asarray(y_arrays)\n",
    "#     y_err = np.asarray(y_err_arrays)\n",
    "    \n",
    "#     # If a single dataset is passed, add a dimension to make it 2D\n",
    "#     if x.ndim == 1:\n",
    "#         x = x[np.newaxis, :]\n",
    "#         y = y[np.newaxis, :]\n",
    "#         y_err = y_err[np.newaxis, :]\n",
    "\n",
    "#     # Get the number of points per fit (n)\n",
    "#     n_points = x.shape[1]\n",
    "#     if n_points < 2:\n",
    "#         nan_array = np.full(x.shape[0], np.nan)\n",
    "#         return nan_array, nan_array, nan_array, nan_array\n",
    "\n",
    "#     # --- Step 1: Calculate Weights and Weighted Sums ---\n",
    "#     # The weight of each point is the inverse of its variance.\n",
    "#     # Handle cases where variance is zero to avoid division errors.\n",
    "#     weights = np.divide(1.0, y_err, out=np.zeros_like(y_err, dtype=float), where=y_err!=0)\n",
    "\n",
    "#     # Calculate the necessary weighted sums for all datasets at once.\n",
    "#     sw = np.sum(weights, axis=1)\n",
    "#     swx = np.sum(weights * x, axis=1)\n",
    "#     swy = np.sum(weights * y, axis=1)\n",
    "#     swxx = np.sum(weights * x**2, axis=1)\n",
    "#     swxy = np.sum(weights * x * y, axis=1)\n",
    "\n",
    "#     # --- Step 2: Calculate Slope and Intercept ---\n",
    "#     # This term is the determinant of the design matrix, used in several calculations.\n",
    "#     delta = sw * swxx - swx**2\n",
    "\n",
    "#     # Calculate slopes (b1) and intercepts (b0) for each dataset.\n",
    "#     # Use np.divide to safely handle cases where delta is zero (e.g., all x are identical).\n",
    "#     slopes = np.divide(sw * swxy - swx * swy, delta, out=np.full_like(delta, np.nan), where=delta!=0)\n",
    "#     intercepts = np.divide(swxx * swy - swx * swxy, delta, out=np.full_like(delta, np.nan), where=delta!=0)\n",
    "\n",
    "#     # --- Step 3: Calculate Standard Errors ---\n",
    "#     # The standard errors are calculated assuming the input variances are known.\n",
    "    \n",
    "#     # Variance of the slope\n",
    "#     var_slopes = np.divide(sw, delta, out=np.full_like(delta, np.nan), where=delta!=0)\n",
    "#     se_slopes = np.sqrt(var_slopes)\n",
    "\n",
    "#     # Variance of the intercept\n",
    "#     var_intercepts = np.divide(swxx, delta, out=np.full_like(delta, np.nan), where=delta!=0)\n",
    "#     se_intercepts = np.sqrt(var_intercepts)\n",
    "\n",
    "#     return slopes, intercepts, se_slopes, se_intercepts\n",
    "\n",
    "\n",
    "def glm_growth_rate_regression_with_offset(t_arrays,\n",
    "                                           n_arrays,\n",
    "                                           N_arrays,\n",
    "                                           pop_size_arrays):\n",
    "    \"\"\"\n",
    "    Compute growth rates (slopes) using a GLM with a population size offset.\n",
    "\n",
    "    This function models the growth of the *absolute abundance* of a genotype,\n",
    "    which is defined as frequency * population_size. It assumes the absolute\n",
    "    abundance grows exponentially.\n",
    "\n",
    "    The model fits ln(E[f] * pop_size) = b0 + b1*t, where f = n/N. This is\n",
    "    achieved by using -ln(pop_size) as an offset in the GLM's linear predictor.\n",
    "\n",
    "    params:\n",
    "        t_arrays (np.ndarray): 2D array of time points, shape (n_datasets, n_points).\n",
    "        n_arrays (np.ndarray): 2D array of observed counts (reads for a genotype),\n",
    "            shape (n_datasets, n_points).\n",
    "        N_arrays (np.ndarray): 2D array of total counts (total reads),\n",
    "            shape (n_datasets, n_points).\n",
    "        pop_size_arrays (np.ndarray): 2D array of the total population size\n",
    "            (e.g., CFU/mL) at each time point, shape (n_datasets, n_points).\n",
    "\n",
    "    returns:\n",
    "        tuple: A tuple containing four 1D NumPy arrays:\n",
    "            - slopes (np.ndarray): The calculated growth rate (k) for each dataset.\n",
    "            - intercepts (np.ndarray): The calculated intercept for each dataset.\n",
    "            - se_slopes (np.ndarray): The standard error of the growth rate.\n",
    "            - se_intercepts (np.ndarray): The standard error of the intercept.\n",
    "    \"\"\"\n",
    "    # --- Step 0: Data Preparation ---\n",
    "    t = np.asarray(t_arrays)\n",
    "    n = np.asarray(n_arrays)\n",
    "    N = np.asarray(N_arrays)\n",
    "    pop_size = np.asarray(pop_size_arrays)\n",
    "\n",
    "    if t.ndim == 1:\n",
    "        t = t[np.newaxis, :]\n",
    "        n = n[np.newaxis, :]\n",
    "        N = N[np.newaxis, :]\n",
    "        pop_size = pop_size[np.newaxis, :]\n",
    "\n",
    "    n_datasets = t.shape[0]\n",
    "\n",
    "    # --- Step 1: Pre-allocate Result Arrays ---\n",
    "    slopes = np.full(n_datasets, np.nan)\n",
    "    intercepts = np.full(n_datasets, np.nan)\n",
    "    se_slopes = np.full(n_datasets, np.nan)\n",
    "    se_intercepts = np.full(n_datasets, np.nan)\n",
    "\n",
    "    # --- Step 2: Loop and Fit GLM for Each Dataset ---\n",
    "    for i in range(n_datasets):\n",
    "        t_i = t[i, :]\n",
    "        n_i = n[i, :]\n",
    "        N_i = N[i, :]\n",
    "        pop_size_i = pop_size[i, :]\n",
    "\n",
    "        # --- Data Validation ---\n",
    "        # Filter for points with valid sequencing and population data.\n",
    "        valid_mask = (N_i > 0) & (n_i <= N_i) & (pop_size_i > 0)\n",
    "        if np.sum(valid_mask) < 2:\n",
    "            continue\n",
    "\n",
    "        t_filt = t_i[valid_mask]\n",
    "        n_filt = n_i[valid_mask]\n",
    "        N_filt = N_i[valid_mask]\n",
    "        pop_size_filt = pop_size_i[valid_mask]\n",
    "        \n",
    "        if np.all(n_filt == 0):\n",
    "            continue\n",
    "\n",
    "        # --- Prepare data for statsmodels ---\n",
    "        endog = np.column_stack((n_filt, N_filt - n_filt))\n",
    "        exog = sm.add_constant(t_filt)\n",
    "\n",
    "        # --- Calculate the offset term ---\n",
    "        # The offset is -ln(pop_size) to model ln(freq * pop_size).\n",
    "        offset_term = -np.log(pop_size_filt)\n",
    "\n",
    "        # --- Step 3: Fit the GLM with the offset ---\n",
    "        try:\n",
    "            glm_binom = sm.GLM(\n",
    "                endog,\n",
    "                exog,\n",
    "                family=sm.families.Binomial(link=sm.families.links.Log()),\n",
    "                offset=offset_term\n",
    "            )\n",
    "            results = glm_binom.fit()\n",
    "\n",
    "            # --- Step 4: Store Results ---\n",
    "            intercepts[i] = results.params[0]\n",
    "            slopes[i] = results.params[1]\n",
    "            se_intercepts[i] = results.bse[0]\n",
    "            se_slopes[i] = results.bse[1]\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return slopes, intercepts, se_slopes, se_intercepts\n",
    "\n",
    "\n",
    "def get_growth_rates(obs_csv):\n",
    "    \n",
    "    print(\"Reading dataframe\",flush=True)\n",
    "    df = pd.read_csv(obs_csv)\n",
    "    clean_df = df.loc[df[\"cfu_per_mL\"] > 0,:].copy()\n",
    "    clean_df.loc[:,\"condition\"] = (clean_df['genotype'].astype(str) + '_' +\n",
    "                                   clean_df['selector'].astype(str) + '_' +\n",
    "                                   clean_df['iptg'].astype(str))\n",
    "    \n",
    "    print(\"Grouping by genotype/selector/time\",flush=True)\n",
    "    count_df = clean_df.reset_index().groupby(\"condition\", sort=False).agg(\n",
    "        first_index=('index', 'first'),\n",
    "        count=('condition', 'size'),\n",
    "        max_count=('count','max'),\n",
    "        max_freq=('freq','max'),\n",
    "        max_cfu_per_mL=('cfu_per_mL','max')\n",
    "    )\n",
    "\n",
    "    return clean_df\n",
    "        \n",
    "    print(\"Creating output dataframe\",flush=True)\n",
    "    out_df = clean_df.loc[count_df[\"first_index\"],[\"condition\",\"genotype\",\"selector\",\"iptg\"]]\n",
    "    out_df[\"n\"] = np.array(count_df[\"count\"])\n",
    "    out_df[\"max_count\"] = np.array(count_df[\"max_count\"])\n",
    "    out_df[\"max_freq\"] = np.array(count_df[\"max_freq\"])\n",
    "    out_df[\"max_cfu_per_mL\"] = np.array(count_df[\"max_cfu_per_mL\"])\n",
    "    out_df[\"slope\"] = np.nan\n",
    "    out_df[\"slope_se\"] = np.nan\n",
    "    out_df[\"intercept\"] = np.nan\n",
    "    out_df[\"intercept_se\"] = np.nan\n",
    "    \n",
    "    all_num_points_seen = np.unique(count_df[\"count\"])\n",
    "    all_num_points_seen.sort()\n",
    "    all_num_points_seen = all_num_points_seen[::-1]\n",
    "    \n",
    "    for n in all_num_points_seen:\n",
    "        if n > 1:\n",
    "            print(f\"fitting data with n = {n} non-zero time points.\")\n",
    "            conditions_to_grab = count_df.index[count_df[\"count\"] == n]\n",
    "            \n",
    "            this_n_mask = clean_df[\"condition\"].isin(conditions_to_grab)\n",
    "            times = np.array(clean_df.loc[this_n_mask,\"time\"])\n",
    "            times = times.reshape((times.shape[0]//n,n))\n",
    "            \n",
    "            ln_cfu = np.log(np.array(clean_df.loc[this_n_mask,\"cfu_per_mL\"]))\n",
    "            ln_cfu = ln_cfu.reshape((ln_cfu.shape[0]//n,n))\n",
    "    \n",
    "            m, b, m_se, b_se = fast_linear_regression(times, ln_cfu)\n",
    "    \n",
    "            out_n_mask = out_df[\"condition\"].isin(conditions_to_grab)\n",
    "            out_df.loc[out_n_mask,\"slope\"] = m\n",
    "            out_df.loc[out_n_mask,\"slope_se\"] = m_se\n",
    "            out_df.loc[out_n_mask,\"intercept\"] = b\n",
    "            out_df.loc[out_n_mask,\"intercept_se\"] = b_se\n",
    "\n",
    "    out_df = out_df.drop(columns=[\"condition\"])\n",
    "\n",
    "    return out_df\n",
    "\n",
    "#growth_df = get_growth_rates(\"many-state_wider/obs_many-state_molar_slower.csv\")\n",
    "            \n",
    "obs_df = pd.read_csv(\"many-state_wider/obs_many-state_molar_slower.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d5a2032-742d-4026-a75f-4d154037157b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>genotype</th>\n",
       "      <th>selector</th>\n",
       "      <th>iptg</th>\n",
       "      <th>time</th>\n",
       "      <th>count</th>\n",
       "      <th>freq</th>\n",
       "      <th>cfu_per_mL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>wt</td>\n",
       "      <td>kan</td>\n",
       "      <td>0.001</td>\n",
       "      <td>30</td>\n",
       "      <td>116709</td>\n",
       "      <td>3.071289e-03</td>\n",
       "      <td>947.349754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>wt</td>\n",
       "      <td>kan</td>\n",
       "      <td>0.001</td>\n",
       "      <td>60</td>\n",
       "      <td>116465</td>\n",
       "      <td>3.064868e-03</td>\n",
       "      <td>949.136594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>wt</td>\n",
       "      <td>kan</td>\n",
       "      <td>0.001</td>\n",
       "      <td>90</td>\n",
       "      <td>115579</td>\n",
       "      <td>3.041553e-03</td>\n",
       "      <td>945.775348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>wt</td>\n",
       "      <td>kan</td>\n",
       "      <td>0.010</td>\n",
       "      <td>30</td>\n",
       "      <td>116806</td>\n",
       "      <td>3.073842e-03</td>\n",
       "      <td>948.958706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>wt</td>\n",
       "      <td>kan</td>\n",
       "      <td>0.010</td>\n",
       "      <td>60</td>\n",
       "      <td>116305</td>\n",
       "      <td>3.060658e-03</td>\n",
       "      <td>949.492043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10794715</th>\n",
       "      <td>10794715</td>\n",
       "      <td>K59Y/M98Y</td>\n",
       "      <td>pheS</td>\n",
       "      <td>10.000</td>\n",
       "      <td>60</td>\n",
       "      <td>17</td>\n",
       "      <td>4.473684e-07</td>\n",
       "      <td>0.139967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10794716</th>\n",
       "      <td>10794716</td>\n",
       "      <td>K59Y/M98Y</td>\n",
       "      <td>pheS</td>\n",
       "      <td>10.000</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>2.894737e-07</td>\n",
       "      <td>0.093189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10794717</th>\n",
       "      <td>10794717</td>\n",
       "      <td>K59Y/M98Y</td>\n",
       "      <td>pheS</td>\n",
       "      <td>30.000</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>2.105263e-07</td>\n",
       "      <td>0.064108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10794718</th>\n",
       "      <td>10794718</td>\n",
       "      <td>K59Y/M98Y</td>\n",
       "      <td>pheS</td>\n",
       "      <td>30.000</td>\n",
       "      <td>60</td>\n",
       "      <td>13</td>\n",
       "      <td>3.421053e-07</td>\n",
       "      <td>0.105062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10794719</th>\n",
       "      <td>10794719</td>\n",
       "      <td>K59Y/M98Y</td>\n",
       "      <td>pheS</td>\n",
       "      <td>30.000</td>\n",
       "      <td>90</td>\n",
       "      <td>10</td>\n",
       "      <td>2.631579e-07</td>\n",
       "      <td>0.082534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10794720 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0   genotype selector    iptg  time   count          freq  \\\n",
       "0                  0         wt      kan   0.001    30  116709  3.071289e-03   \n",
       "1                  1         wt      kan   0.001    60  116465  3.064868e-03   \n",
       "2                  2         wt      kan   0.001    90  115579  3.041553e-03   \n",
       "3                  3         wt      kan   0.010    30  116806  3.073842e-03   \n",
       "4                  4         wt      kan   0.010    60  116305  3.060658e-03   \n",
       "...              ...        ...      ...     ...   ...     ...           ...   \n",
       "10794715    10794715  K59Y/M98Y     pheS  10.000    60      17  4.473684e-07   \n",
       "10794716    10794716  K59Y/M98Y     pheS  10.000    90      11  2.894737e-07   \n",
       "10794717    10794717  K59Y/M98Y     pheS  30.000    30       8  2.105263e-07   \n",
       "10794718    10794718  K59Y/M98Y     pheS  30.000    60      13  3.421053e-07   \n",
       "10794719    10794719  K59Y/M98Y     pheS  30.000    90      10  2.631579e-07   \n",
       "\n",
       "          cfu_per_mL  \n",
       "0         947.349754  \n",
       "1         949.136594  \n",
       "2         945.775348  \n",
       "3         948.958706  \n",
       "4         949.492043  \n",
       "...              ...  \n",
       "10794715    0.139967  \n",
       "10794716    0.093189  \n",
       "10794717    0.064108  \n",
       "10794718    0.105062  \n",
       "10794719    0.082534  \n",
       "\n",
       "[10794720 rows x 8 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec1c63-781e-4bc6-b8f6-4d752634e60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_base_growth_rates(base_dir=\".\"):\n",
    "    \"\"\"\n",
    "    THIS IS A HACKED FUNCTION. \n",
    "    \"\"\"\n",
    "    \n",
    "    base_growth_rates = {}\n",
    "    \n",
    "    for selector in [\"kan\",\"pheS\"]:\n",
    "    \n",
    "        df = pd.read_csv(f\"{base_dir}/{selector}_lib_file.csv\")\n",
    "        for i in tqdm(df.index):    \n",
    "            clone = str(df.loc[i,\"clone\"]).strip()[1:-1].strip()\n",
    "\n",
    "            if clone == \"\":\n",
    "                clone = \"wt\"\n",
    "            \n",
    "            clone = str(re.sub(\"'\",\"\",clone))\n",
    "            clone = clone.split(\",\")\n",
    "            clone = [c.strip() for c in clone]\n",
    "            clone = \"/\".join(clone)\n",
    "\n",
    "            p0g = df.loc[i,f\"{selector}_p0g\"]\n",
    "            p1g = df.loc[i,f\"{selector}_p1g\"]\n",
    "\n",
    "            if clone not in base_growth_rates:\n",
    "                base_growth_rates[clone] = {}\n",
    "\n",
    "            obs = str(df.loc[i,\"obs\"])\n",
    "            obs = np.array([float(v) for v in obs.strip()[1:-1].split()])\n",
    "            \n",
    "\n",
    "            base_growth_rates[clone][selector] = {\"p0g\":p0g,\n",
    "                                                  \"p1g\":p1g,\n",
    "                                                  \"obs\":obs}\n",
    "\n",
    "    sort_by = []\n",
    "    for clone in base_growth_rates:\n",
    "        \n",
    "        if clone == \"wt\":\n",
    "            sort_indexer = (0,-1,-1,clone)\n",
    "        else:\n",
    "            these_muts = clone.split(\"/\")\n",
    "            sort_indexer = [len(these_muts),-1,-1,clone]\n",
    "            for i in range(len(these_muts)):\n",
    "                sort_indexer[i+1] = int(these_muts[i][1:-1])\n",
    "\n",
    "        sort_by.append(tuple(sort_indexer))\n",
    "\n",
    "    sort_by.sort()\n",
    "\n",
    "    out_dict = {\"genotype\":[],\n",
    "                \"selector\":[],\n",
    "                \"p0g\":[],\n",
    "                \"p1g\":[]}\n",
    "    \n",
    "    for g in tqdm(sort_by):\n",
    "        genotype = g[-1]\n",
    "        for selector in [\"kan\",\"pheS\"]:\n",
    "            out_dict[\"genotype\"].append(genotype)\n",
    "            out_dict[\"selector\"].append(selector)\n",
    "            for p in [\"p0g\",\"p1g\"]:\n",
    "                out_dict[p].append(base_growth_rates[genotype][selector][p])\n",
    "            \n",
    "            if len(out_dict) == 4:\n",
    "                num_values = len(base_growth_rates[genotype][selector][\"obs\"])\n",
    "                for i in range(num_values):\n",
    "                    out_dict[f\"obs_{i}\"] = []\n",
    "            for i in range(len(base_growth_rates[genotype][selector][\"obs\"])):\n",
    "                out_dict[f\"obs_{i}\"].append(base_growth_rates[genotype][selector][\"obs\"][i])\n",
    "            \n",
    "            \n",
    "    return pd.DataFrame(out_dict)\n",
    "\n",
    "        \n",
    "base_growth_rates = get_base_growth_rates(\"many-state_wider/\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2fca2f-2130-4c31-a6f6-b88dbd2c242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dataprob\n",
    "import copy\n",
    "\n",
    "def hill_model(lnK,n,lower,upper,x):\n",
    "\n",
    "    x = x**n\n",
    "    K = (np.exp(lnK))**n\n",
    "    shift = upper - lower\n",
    "\n",
    "    return lower + shift*(x/(K + x))\n",
    "\n",
    "fit_parameters = {\"lnK\":  {\"guess\":1},\n",
    "                  \"n\":    {\"guess\":1,\n",
    "                           \"lower_bound\":-10,\n",
    "                           \"upper_bound\":10},\n",
    "                  \"lower\":{\"guess\":0},\n",
    "                  \"upper\":{\"guess\":1}}\n",
    "\n",
    "colors = {\"kan\":\"darkorange\",\n",
    "          \"pheS\":\"darkgreen\"}\n",
    "\n",
    "\n",
    "def fit_dataset(g,growth_df,base_growth_rates,fig=None,ax=None):\n",
    "\n",
    "    g_df = growth_df.loc[growth_df[\"genotype\"] == g,:]\n",
    "    g_bgr_df = base_growth_rates.loc[base_growth_rates[\"genotype\"] == g,:]\n",
    "\n",
    "    all_x = []\n",
    "    all_y = []\n",
    "    all_yerr = []\n",
    "\n",
    "    fit_results = {}\n",
    "                   \n",
    "    for s in pd.unique(g_df[\"selector\"]):\n",
    "        \n",
    "        gs_df = g_df.loc[g_df[\"selector\"] == s,:]\n",
    "        gs_bgr_df = g_bgr_df.loc[g_bgr_df[\"selector\"] == s,:]\n",
    "\n",
    "        m = np.array(gs_df[\"slope\"])\n",
    "        m_err = np.array(gs_df[\"slope_se\"])\n",
    "\n",
    "        if s == \"kan\":\n",
    "            min_base = np.array(gs_bgr_df[\"p0g\"])[0]\n",
    "            max_base = np.array(gs_bgr_df[\"p1g\"])[0]\n",
    "            normalized = (m - min_base)/(max_base - min_base)\n",
    "            normalized_se = (m_err - min_base)/(max_base - min_base)\n",
    "        else:\n",
    "            min_base = np.array(gs_bgr_df[\"p0g\"])[0]\n",
    "            max_base = np.array(gs_bgr_df[\"p1g\"])[0]\n",
    "            normalized = (m - min_base)/(max_base - min_base)\n",
    "            normalized_se = (m_err - max_base)/(min_base - max_base)\n",
    "\n",
    "        normalized_se[np.isnan(normalized_se)] = np.abs(max_base - min_base)*0.1\n",
    "        normalized_se = np.abs(normalized_se)\n",
    "        \n",
    "        x = np.array(gs_df[\"iptg\"])\n",
    "\n",
    "        m = dataprob.setup(hill_model,\n",
    "                           fit_parameters=copy.deepcopy(fit_parameters),\n",
    "                           non_fit_kwargs={\"x\":x})\n",
    "        m.fit(y_obs=normalized,\n",
    "              y_std=normalized_se)\n",
    "\n",
    "        fit_results[s] = m.fit_df.copy()\n",
    "        \n",
    "        if ax is not None:\n",
    "            \n",
    "            ax.scatter(x,normalized,s=100,fc=colors[s])\n",
    "            ax.errorbar(x=x,\n",
    "                        y=normalized,\n",
    "                        yerr=normalized_se,\n",
    "                        color=colors[s],lw=0,elinewidth=1,capsize=5)\n",
    "            ax.plot(x,m.model(m.fit_df[\"estimate\"]),color=colors[s],lw=3)\n",
    "\n",
    "        all_x.extend(list(x))\n",
    "        all_y.extend(list(normalized))\n",
    "        all_yerr.extend(list(normalized_se))\n",
    "\n",
    "    all_x = np.array(all_x)\n",
    "    idx = np.argsort(all_x)\n",
    "    all_x = all_x[idx]\n",
    "    \n",
    "    m = dataprob.setup(hill_model,\n",
    "                       fit_parameters=copy.deepcopy(fit_parameters),\n",
    "                       non_fit_kwargs={\"x\":all_x})\n",
    "    \n",
    "    m.fit(y_obs=np.array(all_y)[idx],\n",
    "          y_std=np.array(all_yerr)[idx])\n",
    "    \n",
    "    fit_results[\"all\"] = m.fit_df.copy()\n",
    "\n",
    "    if ax is not None:\n",
    "        ax.plot(all_x,m.model(m.fit_df[\"estimate\"]),color=\"skyblue\",lw=3)\n",
    "\n",
    "    \n",
    "    obs_keys = np.array([f\"obs_{i}\" for i in range(8)])\n",
    "    y = np.array(gs_bgr_df.loc[:,obs_keys]).flatten()*12\n",
    "    m = dataprob.setup(hill_model,\n",
    "                       fit_parameters=copy.deepcopy(fit_parameters),\n",
    "                       non_fit_kwargs={\"x\":x})\n",
    "    \n",
    "    m.fit(y_obs=y,y_std=np.ones(len(y))*0.01)\n",
    "    fit_results[\"input\"] = m.fit_df.copy()\n",
    "\n",
    "    input_fit = m.fit_df.copy()\n",
    "\n",
    "    if ax is not None:\n",
    "        \n",
    "        ax.plot(x,y,color=\"blue\",lw=3)\n",
    "    \n",
    "        ax.set_title(g)\n",
    "        ax.set_xscale(\"log\")\n",
    "        plt.show()\n",
    "\n",
    "    return fit_results\n",
    "\n",
    "\n",
    "\n",
    "out_dict = {}\n",
    "for key in [\"genotype\",\"fit_on\",\"lnK\",\"lnK_std\",\"n\",\"n_std\",\"lower\",\"lower_std\",\"upper\",\"upper_std\"]:\n",
    "    out_dict[key] = []\n",
    "\n",
    "counter = 0\n",
    "for g in tqdm(pd.unique(growth_df[\"genotype\"])):\n",
    "\n",
    "    counter += 1\n",
    "    if counter > 200:\n",
    "         break\n",
    "\n",
    "    fig, ax = plt.subplots(1,figsize=(6,6))\n",
    "    #fig, ax = None, None\n",
    "    \n",
    "    fit_results = fit_dataset(g,\n",
    "                              growth_df,\n",
    "                              base_growth_rates,\n",
    "                              fig=fig,\n",
    "                              ax=ax)\n",
    "\n",
    "    break\n",
    "    for fit_on in [\"kan\",\"pheS\",\"all\",\"input\"]:\n",
    "\n",
    "        lnK, lnK_std, n, n_std, lower, lower_std, upper, upper_std = np.array(fit_results[fit_on].loc[[\"lnK\",\"n\",\"lower\",\"upper\"],[\"estimate\",\"std\"]]).ravel()\n",
    "\n",
    "        out_dict[\"genotype\"].append(g)\n",
    "        out_dict[\"fit_on\"].append(fit_on)\n",
    "        out_dict[\"lnK\"].append(lnK)\n",
    "        out_dict[\"lnK_std\"].append(lnK_std)\n",
    "        out_dict[\"n\"].append(n)\n",
    "        out_dict[\"n_std\"].append(n_std)\n",
    "        out_dict[\"lower\"].append(lower)\n",
    "        out_dict[\"lower_std\"].append(lower_std)\n",
    "        out_dict[\"upper\"].append(upper)\n",
    "        out_dict[\"upper_std\"].append(upper_std)\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edd9caa-459d-4a44-99ca-a00510e4eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(out_dict)\n",
    "input_df = results_df.loc[results_df[\"fit_on\"] == \"input\",:]\n",
    "all_df = results_df.loc[results_df[\"fit_on\"] == \"all\",:]\n",
    "\n",
    "check_estimate = \"lower\"\n",
    "\n",
    "fig, ax = plt.subplots(1,figsize=(6,6))\n",
    "\n",
    "ax.errorbar(x=input_df[check_estimate],\n",
    "            #xerr=input_df[f\"{check_estimate}_std\"],\n",
    "            y=all_df[check_estimate],\n",
    "            #yerr=all_df[f\"{check_estimate}_std\"],\n",
    "            \n",
    "            lw=0,capsize=5,elinewidth=1,ms=5,marker=\"o\",markerfacecolor=\"none\")\n",
    "\n",
    "ax.set_xlim(-5,5)\n",
    "ax.set_ylim(-5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac3b5f3-d59f-452c-a97f-5fea17a6a4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_series(y_obs,y_err,m):\n",
    "\n",
    "    m.fit(y_obs=y_obs,\n",
    "          y_std=y_err)\n",
    "\n",
    "    return m.fit_df\n",
    "\n",
    "def get_fitnesses(growth_df):\n",
    "\n",
    "    selectors = np.unique(growth_df[\"selector\"])\n",
    "    selectors.sort()\n",
    "\n",
    "    guesses = {}\n",
    "    for s in selectors:\n",
    "        slopes = np.array(df.loc[df[\"selector\"] == s,\"slope\"])\n",
    "        upper = np.mean(slopes[slopes > 0])\n",
    "        lower = np.mean(slopes[slopes <= 0])\n",
    "\n",
    "\n",
    "\n",
    "    upper_guess = np.mean(growth_df.loc[growth_df[\"slope\"] > 0,\"slope\"])\n",
    "    lower_guess = np.mean(growth_df.loc[growth_df[\"slope\"] <= 0,\"slope\"])\n",
    "\n",
    "get_fitnesses(growth_df)\n",
    "\n",
    "x = np.arange(0,100,10)\n",
    "y = hill_model(np.log(20),1,lower=lower,upper=upper,x=x)\n",
    "y_obs = y + np.random.normal(0,0.01,len(y))\n",
    "y_err = np.ones(len(y_obs))*0.01\n",
    "\n",
    "\n",
    "\n",
    "fit_series(y_obs,y_err,m=m)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plt.plot(x,y)\n",
    "# m.fit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6811cfd-8df2-4e44-b193-0f78797af76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to get results_fitness_full from screen\n",
    "#There are two options one function does this vectorized which is quicker but hard on memory the other chunks the data taking longer but this revents crashing \n",
    "# Usage:\n",
    "# results_df = calculate_fitness_smart(df) ->this asks how much memory does this machine have to decide which method to use \n",
    "# Or explicitly choose one:\n",
    "# results_df = calculate_fitness_vectorized(df)  # Fastest, needs more memory <-Nat's laptop can do this \n",
    "# results_df = calculate_fitness_chunked(df)     # Slower, memory efficient\n",
    "def calculate_fitness_vectorized(df):\n",
    "    \"\"\"\n",
    "    Vectorized fitness calculation for millions of mutants\n",
    "    Much faster than nested loops approach\n",
    "    Dataframe should have df.columns: 'genotype', 'selector', 'iptg', 'time', 'count', 'freq','cfu_per_mL', 'ln_cfu'\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Starting vectorized fitness calculation...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Group by the combination we need to fit\n",
    "    grouped = df.groupby(['genotype', 'selector', 'iptg'])\n",
    "    \n",
    "    # Pre-allocate lists for results (more efficient than appending)\n",
    "    n_groups = len(grouped)\n",
    "    results = {\n",
    "        'seq': [],\n",
    "        'iptg': [],\n",
    "        'selection_condition': [],\n",
    "        'fitness': [],\n",
    "        'residual': [],\n",
    "        'r_squared': [],\n",
    "        'n_points': []\n",
    "    }\n",
    "    \n",
    "    print(f\"Processing {n_groups} unique combinations...\")\n",
    "    \n",
    "    # Process all groups\n",
    "    group_count = 0\n",
    "    for (genotype, selector, iptg), group_df in grouped: #creates subsets of group_df with matching (genotype, selector, iptg) combos\n",
    "        if len(group_df) < 2:  # Need at least 2 points for linear fit\n",
    "            continue\n",
    "            \n",
    "        # Extract time and ln_cfu as numpy arrays (faster than multiple indexing)\n",
    "        time_vals = group_df[\"time\"].values\n",
    "        ln_cfu_vals = group_df[\"ln_cfu\"].values\n",
    "        \n",
    "        # Use scipy.stats.linregress - faster than np.polynomial.Polynomial.fit\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(time_vals, ln_cfu_vals)\n",
    "        \n",
    "        # Calculate residual sum of squares manually (faster than full=True)\n",
    "        predicted = slope * time_vals + intercept\n",
    "        residual_ss = np.sum((ln_cfu_vals - predicted) ** 2)\n",
    "        \n",
    "        # Store results\n",
    "        results['seq'].append(genotype)\n",
    "        results['iptg'].append(iptg)\n",
    "        results['selection_condition'].append(selector)\n",
    "        results['fitness'].append(slope)\n",
    "        results['residual'].append(residual_ss)\n",
    "        results['r_squared'].append(r_value ** 2)\n",
    "        results['n_points'].append(len(group_df))\n",
    "        \n",
    "        group_count += 1\n",
    "        if group_count % 50000 == 0:  # Progress indicator\n",
    "            print(f\"Processed {group_count:,} / {n_groups:,} combinations...\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Fitness calculation completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Processed {group_count:,} valid combinations\")\n",
    "    \n",
    "    # Create DataFrame from results (much faster than pd.DataFrame(list_of_dicts))\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def calculate_fitness_chunked(df, chunk_size=100000):\n",
    "    \"\"\"\n",
    "    Memory-efficient version that processes data in chunks\n",
    "    Use this if you run out of memory with the regular version\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Starting chunked fitness calculation...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get unique combinations\n",
    "    combinations = df.groupby(['genotype', 'selector', 'iptg']).size().reset_index(name='count')\n",
    "    total_combinations = len(combinations)\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # Process in chunks\n",
    "    for chunk_start in range(0, total_combinations, chunk_size):\n",
    "        chunk_end = min(chunk_start + chunk_size, total_combinations)\n",
    "        chunk_combinations = combinations.iloc[chunk_start:chunk_end]\n",
    "        \n",
    "        print(f\"Processing chunk {chunk_start:,} to {chunk_end:,} / {total_combinations:,}\")\n",
    "        \n",
    "        chunk_results = {\n",
    "            'seq': [],\n",
    "            'iptg': [],\n",
    "            'selection_condition': [],\n",
    "            'fitness': [],\n",
    "            'residual': [],\n",
    "            'r_squared': [],\n",
    "            'n_points': []\n",
    "        }\n",
    "        \n",
    "        for _, row in chunk_combinations.iterrows():\n",
    "            genotype, selector, iptg = row['genotype'], row['selector'], row['iptg']\n",
    "            \n",
    "            # Filter data for this combination\n",
    "            mask = (df['genotype'] == genotype) & (df['selector'] == selector) & (df['iptg'] == iptg)\n",
    "            group_df = df[mask]\n",
    "            \n",
    "            if len(group_df) < 2:\n",
    "                continue\n",
    "                \n",
    "            # Linear regression\n",
    "            time_vals = group_df[\"time\"].values\n",
    "            ln_cfu_vals = group_df[\"ln_cfu\"].values\n",
    "            \n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(time_vals, ln_cfu_vals)\n",
    "            \n",
    "            # Calculate residual\n",
    "            predicted = slope * time_vals + intercept\n",
    "            residual_ss = np.sum((ln_cfu_vals - predicted) ** 2)\n",
    "            \n",
    "            # Store results\n",
    "            chunk_results['seq'].append(genotype)\n",
    "            chunk_results['iptg'].append(iptg)\n",
    "            chunk_results['selection_condition'].append(selector)\n",
    "            chunk_results['fitness'].append(slope)\n",
    "            chunk_results['residual'].append(residual_ss)\n",
    "            chunk_results['r_squared'].append(r_value ** 2)\n",
    "            chunk_results['n_points'].append(len(group_df))\n",
    "        \n",
    "        # Convert chunk to DataFrame and store\n",
    "        chunk_df = pd.DataFrame(chunk_results)\n",
    "        all_results.append(chunk_df)\n",
    "    \n",
    "    # Combine all chunks\n",
    "    results_df = pd.concat(all_results, ignore_index=True)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Chunked calculation completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Total results: {len(results_df):,}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Choose the appropriate function based on your memory constraints\n",
    "def calculate_fitness_smart(df, memory_limit_gb=8):\n",
    "    \"\"\"\n",
    "    Automatically choose between vectorized and chunked approach based on data size\n",
    "    \"\"\"\n",
    "    \n",
    "    # Estimate memory usage (rough calculation)\n",
    "    estimated_memory_gb = len(df) * len(df.columns) * 8 / (1024**3)  # 8 bytes per float64\n",
    "    \n",
    "    print(f\"Estimated memory usage: {estimated_memory_gb:.2f} GB\")\n",
    "    \n",
    "    if estimated_memory_gb > memory_limit_gb:\n",
    "        print(\"Using chunked approach due to memory constraints...\")\n",
    "        return calculate_fitness_chunked(df)\n",
    "    else:\n",
    "        print(\"Using vectorized approach...\")\n",
    "        return calculate_fitness_vectorized(df)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Functions defined. Usage:\")\n",
    "print(\"results_df = calculate_fitness_smart(df)  # Automatic choice\")\n",
    "print(\"results_df = calculate_fitness_vectorized(df)  # Fastest\")  \n",
    "print(\"results_df = calculate_fitness_chunked(df)  # Memory efficient\")\n",
    "#done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a682950-4ccb-4141-9fbb-2f361c265004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in csv file with seqs and cfu/ml at each iptg conc. \n",
    "#This dataframe has the headers 'Unnamed: 0', 'genotype', 'selector', 'iptg', 'time', 'count', 'freq','cfu_per_mL'\n",
    "df_start = pd.read_csv(\"2025-05-17_simulate-sampling\\output\\output_obs_20250703_NJ_df.csv\")\n",
    "#done\n",
    "df_start = df_start.drop(columns=[\"Unnamed: 0\"])#remove the columns that is just the line number \n",
    "#done\n",
    "df_start[\"ln_cfu\"] = np.log2(df_start[\"cfu_per_mL\"]) #take the log base 2 of the cfu_per_mL - seems to be common\n",
    "#done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27192daf-92dc-41b8-aa18-e9d1686900d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = calculate_fitness_smart(df_start)#this calculates the fitness/growth rate from a line with more than 2 ln cfu per ml over time\n",
    "#returns as df with columns 'seq', 'iptg', 'selection_condition', 'fitness', 'residual', 'r_squared', 'n_points'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57330f13-7f0d-449e-a91b-e1e2d0047b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"results_fitness_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ae18ae-bc04-43c8-ba74-d100fe588dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
